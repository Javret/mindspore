Inicio rápido para principiantes

A continuación se describen las funciones básicas de MindSpore para implementar tareas comunes en el deep learning. Para más detalles, consulte los enlaces en cada sección.

Configuración de la información de ejecución

MindSpore usa context.set_context para configurar la información requerida para la ejecución, como por ejemplo el modo de ejecución, la información del backend y la información del hardware.

Importe el módulo context y configure la información requerida.

import os
import argparse
from mindspore import context

parser = argparse.ArgumentParser(description='MindSpore LeNet Example')
parser.add_argument('--device_target', type=str, default="CPU", choices=['Ascend', 'GPU', 'CPU'])

args = parser.parse_known_args()[0]
context.set_context(mode=context.GRAPH_MODE, device_target=args.device_target)


Este ejemplo se ejecuta en modo gráfico. Puedes configurar la información del hardware según sea necesario. Por ejemplo, si el código se ejecuta en el procesador Ascend AI, establece --device_target a Ascend. Esta regla también se aplica al código que se ejecuta en la CPU y la GPU. Para más detalles sobre los parámetros, véase context.set_context.

Descarga del conjunto de datos

El conjunto de datos MNIST utilizado en este ejemplo consiste en 10 clases de imágenes en escala de grises de 28 x 28 píxeles. Tiene un conjunto de entrenamiento de 60.000 ejemplos y un conjunto de prueba de 10.000 ejemplos.

Haga clic aquí para descargar y descomprimir el conjunto de datos MNIST y colocar el conjunto de datos según la siguiente estructura de directorios. El siguiente código de ejemplo descarga y descomprime el conjunto de datos en la ubicación especificada.

import os
import requests

def download_dataset(dataset_url, path):
    filename = dataset_url.split("/")[-1]
    save_path = os.path.join(path, filename)
    if os.path.exists(save_path):
        return
    if not os.path.exists(path):
        os.makedirs(path)
    res = requests.get(dataset_url, stream=True, verify=False)
    with open(save_path, "wb") as f:
        for chunk in res.iter_content(chunk_size=512):
            if chunk:
                f.write(chunk)

train_path = "datasets/MNIST_Data/train"
test_path = "datasets/MNIST_Data/test"

download_dataset("https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/train-labels-idx1-ubyte", train_path)
download_dataset("https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/train-images-idx3-ubyte", train_path)
download_dataset("https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/t10k-labels-idx1-ubyte", test_path)
download_dataset("https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/t10k-images-idx3-ubyte", test_path)


La estructura de directorios del archivo del conjunto de datos es la siguiente:

    ./datasets/MNIST_Data
    ├── test
    │   ├── t10k-images-idx3-ubyte
    │   └── t10k-labels-idx1-ubyte
    └── train
        ├── train-images-idx3-ubyte
        └── train-labels-idx1-ubyte

    2 directories, 4 files


Procesamiento de datos

Los conjuntos de datos son cruciales para el entrenamiento del modelo. Un buen conjunto de datos puede mejorar de forma efectiva la precisión y la eficiencia del entrenamiento. MindSpore proporciona el módulo API mindspore.dataset para el procesamiento de datos para almacenar muestras y etiquetas. Antes de cargar un conjunto de datos, normalmente procesamos el conjunto de datos. mindspore.dataset integra métodos comunes de procesamiento de datos.

Importe mindspore.dataset y otros módulos correspondientes en MindSpore.

import mindspore.dataset as ds
import mindspore.dataset.transforms.c_transforms as C
import mindspore.dataset.vision.c_transforms as CV
from mindspore.dataset.vision import Inter
from mindspore import dtype as mstype


El procesamiento del conjunto de datos consiste en los siguientes pasos:

1. Definir la función create_dataset para crear un conjunto de datos.

2. Definir las operaciones de aumento y procesamiento de datos para preparar el posterior mapeo.

3. Utilizar la función map para aplicar operaciones de datos al conjunto de datos.

4. Realizar operaciones de barajado y por lotes sobre los datos.

def create_dataset(data_path, batch_size=32, repeat_size=1,
                   num_parallel_workers=1):
    # Define the dataset.
    mnist_ds = ds.MnistDataset(data_path)
    resize_height, resize_width = 32, 32
    rescale = 1.0 / 255.0
    shift = 0.0
    rescale_nml = 1 / 0.3081
    shift_nml = -1 * 0.1307 / 0.3081

    # Define the mapping to be operated.
    resize_op = CV.Resize((resize_height, resize_width), interpolation=Inter.LINEAR)
    rescale_nml_op = CV.Rescale(rescale_nml, shift_nml)
    rescale_op = CV.Rescale(rescale, shift)
    hwc2chw_op = CV.HWC2CHW()
    type_cast_op = C.TypeCast(mstype.int32)

    # Use the map function to apply data operations to the dataset.
    mnist_ds = mnist_ds.map(operations=type_cast_op, input_columns="label", num_parallel_workers=num_parallel_workers)
    mnist_ds = mnist_ds.map(operations=[resize_op, rescale_op, rescale_nml_op, hwc2chw_op], input_columns="image", num_parallel_workers=num_parallel_workers)


    # Perform shuffle, batch and repeat operations.
    buffer_size = 10000
    mnist_ds = mnist_ds.shuffle(buffer_size=buffer_size)
    mnist_ds = mnist_ds.batch(batch_size, drop_remainder=True)
    mnist_ds = mnist_ds.repeat(count=repeat_size)

    return mnist_ds


En la información anterior, batch_size indica el número de registros de datos en cada grupo. Suponga que cada grupo contiene 32 registros de datos.

MindSpore soporta múltiples operaciones de procesamiento y argumentación de datos. Para más detalles, consulte Procesamiento de datos y aumento de datos.

Creación de un modelo
Para utilizar MindSpore para la definición de redes neuronales, herede mindspore.nn.Cell. Cell es la clase base de todas las redes neuronales (como Conv2d-relu-softmax).

Defina cada capa de una red neuronal en el método __init__ por adelantado, y luego defina el método construct para completar la construcción hacia adelante de la red neuronal. De acuerdo con la estructura de LeNet, defina las capas de la red como sigue:

import mindspore.nn as nn
from mindspore.common.initializer import Normal

class LeNet5(nn.Cell):
    """
    Lenet network structure
    """
    def __init__(self, num_class=10, num_channel=1):
        super(LeNet5, self).__init__()
        # Define the required operation.
        self.conv1 = nn.Conv2d(num_channel, 6, 5, pad_mode='valid')
        self.conv2 = nn.Conv2d(6, 16, 5, pad_mode='valid')
        self.fc1 = nn.Dense(16 * 5 * 5, 120, weight_init=Normal(0.02))
        self.fc2 = nn.Dense(120, 84, weight_init=Normal(0.02))
        self.fc3 = nn.Dense(84, num_class, weight_init=Normal(0.02))
        self.relu = nn.ReLU()
        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)
        self.flatten = nn.Flatten()

    def construct(self, x):
        # Use the defined operation to construct a forward network.
        x = self.conv1(x)
        x = self.relu(x)
        x = self.max_pool2d(x)
        x = self.conv2(x)
        x = self.relu(x)
        x = self.max_pool2d(x)
        x = self.flatten(x)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.relu(x)
        x = self.fc3(x)
        return x

# Instantiate the network.
net = LeNet5()

Optimización de los parámetros del modelo
Para entrenar un modelo de red neuronal, es necesario definir una función de pérdida y un optimizador.

Las funciones de pérdida soportadas por MindSpore incluyen SoftmaxCrossEntropyWithLogits, L1Loss, y MSELoss. A continuación se utiliza la función de pérdida de entropía cruzada SoftmaxCrossEntropyWithLogits.

# Define the loss function.
net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')

Para más información sobre el uso de funciones de pérdida en mindspore, vea Funciones de Pérdida.

MindSpore soporta los optimizadores Adam, AdamWeightDecay y Momentum. A continuación se utiliza el optimizador Momentum como ejemplo.

# Define the optimizer.
net_opt = nn.Momentum(net.trainable_params(), learning_rate=0.01, momentum=0.9)


Para más información sobre el uso de un optimizador en mindspore, vea Optimizador.

Entrenando y guardando el modelo
MindSpore proporciona el mecanismo de devolución de llamada para ejecutar la lógica personalizada durante el entrenamiento. A continuación se utiliza ModelCheckpoint proporcionado por el framework como ejemplo. ModelCheckpoint puede guardar el modelo de red y los parámetros para su posterior ajuste.

from mindspore.train.callback import ModelCheckpoint, CheckpointConfig
# Set model saving parameters.
config_ck = CheckpointConfig(save_checkpoint_steps=1875, keep_checkpoint_max=10)
# Use model saving parameters.
ckpoint = ModelCheckpoint(prefix="checkpoint_lenet", config=config_ck)

La API model.train proporcionada por MindSpore puede utilizarse para entrenar fácilmente la red. LossMonitor puede monitorizar los cambios del valor de la pérdida durante el proceso de entrenamiento.

# Import the library required for model training.
from mindspore.nn import Accuracy
from mindspore.train.callback import LossMonitor
from mindspore import Model

def train_net(model, epoch_size, data_path, repeat_size, ckpoint_cb, sink_mode):
    """Define a training method."""
    # Load the training dataset.
    ds_train = create_dataset(os.path.join(data_path, "train"), 32, repeat_size)
    model.train(epoch_size, ds_train, callbacks=[ckpoint_cb, LossMonitor(125)], dataset_sink_mode=sink_mode)


dataset_sink_mode se utiliza para controlar si los datos se descargan. La descarga de datos significa que los datos se transmiten directamente al dispositivo a través de un canal para acelerar la velocidad de entrenamiento. Si dataset_sink_mode es True, los datos se descargan. En caso contrario, los datos no se descargan.

Valide la capacidad de generalización del modelo basándose en el resultado obtenido al ejecutar el conjunto de datos de prueba.

1. Lea el conjunto de datos de prueba utilizando la API model.eval.

2. Utilice los parámetros del modelo guardados para la inferencia.

def test_net(model, data_path):
    """Define a validation method."""
    ds_eval = create_dataset(os.path.join(data_path, "test"))
    acc = model.eval(ds_eval, dataset_sink_mode=False)
    print("{}".format(acc))


Establezca train_epoch en 1 para entrenar el conjunto de datos en una época. En los métodos train_net y test_net, se carga el conjunto de datos de entrenamiento previamente descargado. mnist_path es la ruta del conjunto de datos MNIST.

train_epoch = 1
mnist_path = "./datasets/MNIST_Data"
dataset_size = 1
model = Model(net, net_loss, net_opt, metrics={"Accuracy": Accuracy()})
train_net(model, train_epoch, mnist_path, dataset_size, ckpoint, False)
test_net(model, mnist_path)


Ejecute el siguiente comando para ejecutar el script:

python lenet.py --device_target=CPU


Donde,

lenet.py: Puedes pegar el código anterior en lenet.py (excluyendo el código para descargar el conjunto de datos). En general, puedes mover la parte de importación al principio del código, colocar las definiciones de las clases, funciones y métodos después del código, y conectar las operaciones precedentes en el método principal.

--device_target=CPU: especifica la plataforma de hardware en ejecución. El valor del parámetro puede ser CPU, GPU o Ascend, dependiendo de la plataforma de hardware en ejecución.

Los valores de pérdida se muestran durante el entrenamiento, como se muestra a continuación. Aunque los valores de pérdida pueden fluctuar, disminuyen gradualmente y la precisión aumenta gradualmente en general. Los valores de pérdida mostrados cada vez pueden ser diferentes debido a su aleatoriedad. El siguiente es un ejemplo de los valores de pérdida mostrados durante el entrenamiento:

epoch: 1 step: 125, loss is 2.3083377
epoch: 1 step: 250, loss is 2.3019726

epoch: 1 step: 1500, loss is 0.028385757
epoch: 1 step: 1625, loss is 0.0857362
epoch: 1 step: 1750, loss is 0.05639569
epoch: 1 step: 1875, loss is 0.12366105
{'Accuracy': 0.9663477564102564}


Los datos de precisión del modelo se muestran en el contenido de salida. En el ejemplo, la precisión alcanza el 96,6%, lo que indica una buena calidad del modelo. A medida que el número de épocas de la red (train_epoch) aumenta, la precisión del modelo mejorará aún más.

Cargar el modelo

from mindspore import load_checkpoint, load_param_into_net
# Load the saved model for testing.
param_dict = load_checkpoint("checkpoint_lenet-1_1875.ckpt")
# Load parameters to the network.
load_param_into_net(net, param_dict)

Para más información sobre la carga de un modelo en mindspore, véase Cargar el modelo.

Validación del modelo
Utilice el modelo generado para predecir la clasificación de una sola imagen. El procedimiento es el siguiente:

Las imágenes predecidas se generarán aleatoriamente, y los resultados pueden ser diferentes cada vez.

import numpy as np
from mindspore import Tensor

# Define a test dataset. If batch_size is set to 1, an image is obtained.
ds_test = create_dataset(os.path.join(mnist_path, "test"), batch_size=1).create_dict_iterator()
data = next(ds_test)

# `images` indicates the test image, and `labels` indicates the actual classification of the test image.
images = data["image"].asnumpy()
labels = data["label"].asnumpy()

# Use the model.predict function to predict the classification of the image.
output = model.predict(Tensor(data['image']))
predicted = np.argmax(output.asnumpy(), axis=1)

# Output the predicted classification and the actual classification.
print(f'Predicted: "{predicted[0]}", Actual: "{labels[0]}"')

